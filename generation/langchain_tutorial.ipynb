{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel LangChain avec Ollama\n",
    "\n",
    "\n",
    "Tutoriel : [LangChain for LLM application developpment](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/) de DeepLearning.ai.  \n",
    "\n",
    "\n",
    "### SOMMAIRE  \n",
    "[1. Models, prompts and parsers](#models-prompts-and-parsers)  \n",
    "[2. Memory](#memory)  \n",
    "[3. Chains](#chains)  \n",
    "[4. Question and answer](#question-and-answer)  \n",
    "[5. Evaluation](#evaluation)  \n",
    "[6. Agents](#agents)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Packages requirements list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohappyeyeballs==2.4.0\n",
      "aiohttp==3.10.5\n",
      "aiosignal==1.3.1\n",
      "annotated-types==0.7.0\n",
      "anyio==4.4.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.4.1\n",
      "async-lru==2.0.4\n",
      "attrs==24.2.0\n",
      "babel==2.16.0\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "certifi==2024.8.30\n",
      "cffi==1.17.1\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "comm==0.2.2\n",
      "dataclasses-json==0.6.7\n",
      "debugpy==1.8.5\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "dnspython==2.6.1\n",
      "docarray==0.40.0\n",
      "email_validator==2.2.0\n",
      "executing==2.1.0\n",
      "fastapi==0.115.0\n",
      "fastapi-cli==0.0.5\n",
      "fastjsonschema==2.20.0\n",
      "filelock==3.16.0\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.9.0\n",
      "greenlet==3.0.3\n",
      "h11==0.14.0\n",
      "httpcore==1.0.5\n",
      "httptools==0.6.1\n",
      "httpx==0.27.2\n",
      "huggingface-hub==0.24.7\n",
      "idna==3.8\n",
      "ipykernel==6.29.5\n",
      "ipython==8.27.0\n",
      "ipywidgets==8.1.5\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.4\n",
      "joblib==1.4.2\n",
      "json5==0.9.25\n",
      "jsonpatch==1.33\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter==1.1.1\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.10.0\n",
      "jupyter-lsp==2.2.5\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "jupyter_server==2.14.2\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.2.5\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.3\n",
      "jupyterlab_widgets==3.0.13\n",
      "langchain==0.2.16\n",
      "langchain-community==0.2.16\n",
      "langchain-core==0.2.38\n",
      "langchain-experimental==0.0.64\n",
      "langchain-ollama==0.1.3\n",
      "langchain-postgres==0.0.12\n",
      "langchain-text-splitters==0.2.4\n",
      "langsmith==0.1.131\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.5\n",
      "marshmallow==3.22.0\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mistune==3.0.2\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "mypy-extensions==1.0.0\n",
      "nbclient==0.10.0\n",
      "nbconvert==7.16.4\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.3\n",
      "notebook==7.2.2\n",
      "notebook_shim==0.2.4\n",
      "numexpr==2.10.1\n",
      "numpy==1.26.4\n",
      "ollama==0.3.2\n",
      "orjson==3.10.7\n",
      "overrides==7.7.0\n",
      "packaging==24.1\n",
      "pandas==2.2.2\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "pgvector==0.2.5\n",
      "pillow==10.4.0\n",
      "platformdirs==4.3.2\n",
      "prometheus_client==0.20.0\n",
      "prompt_toolkit==3.0.47\n",
      "psutil==6.0.0\n",
      "psycopg==3.2.2\n",
      "psycopg-binary==3.2.2\n",
      "psycopg-pool==3.2.3\n",
      "pure_eval==0.2.3\n",
      "pycparser==2.22\n",
      "pydantic==2.9.0\n",
      "pydantic_core==2.23.2\n",
      "Pygments==2.18.0\n",
      "pypdf==5.0.0\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.0.1\n",
      "python-json-logger==2.0.7\n",
      "python-multipart==0.0.10\n",
      "pytz==2024.2\n",
      "pywin32==306\n",
      "pywinpty==2.0.13\n",
      "PyYAML==6.0.2\n",
      "pyzmq==26.2.0\n",
      "referencing==0.35.1\n",
      "regex==2024.9.11\n",
      "requests==2.32.3\n",
      "requests-toolbelt==1.0.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.8.1\n",
      "rpds-py==0.20.0\n",
      "safetensors==0.4.5\n",
      "scikit-learn==1.5.2\n",
      "scipy==1.14.1\n",
      "Send2Trash==1.8.3\n",
      "sentence-transformers==3.1.1\n",
      "shellingham==1.5.4\n",
      "six==1.16.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.6\n",
      "SQLAlchemy==2.0.34\n",
      "stack-data==0.6.3\n",
      "starlette==0.38.6\n",
      "sympy==1.13.3\n",
      "tenacity==8.5.0\n",
      "terminado==0.18.1\n",
      "threadpoolctl==3.5.0\n",
      "tinycss2==1.3.0\n",
      "tokenizers==0.19.1\n",
      "torch==2.4.1\n",
      "tornado==6.4.1\n",
      "tqdm==4.66.5\n",
      "traitlets==5.14.3\n",
      "transformers==4.44.2\n",
      "typer==0.12.5\n",
      "types-python-dateutil==2.9.0.20240906\n",
      "types-requests==2.32.0.20240907\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.12.2\n",
      "tzdata==2024.1\n",
      "uri-template==1.3.0\n",
      "urllib3==2.2.2\n",
      "uvicorn==0.31.0\n",
      "watchfiles==0.24.0\n",
      "wcwidth==0.2.13\n",
      "webcolors==24.8.0\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "websockets==13.1\n",
      "widgetsnbextension==4.0.13\n",
      "wikipedia==1.4.0\n",
      "yarl==1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='models-prompts-and-parsers'>1. Models, prompts and parsers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:2b-instruct-q4_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's how to solve this:\\n\\n1. **Find out when Justin Bieber was born:** Justin Bieber was born on March 1, 1994.\\n2. **Find the Super Bowl winner for that year:** The Super Bowl in the year Justin Bieber was born (Super Bowl XXVIII) took place in January 1994.\\n3. **Determine the winning team:** The Atlanta Falcons won Super Bowl XXVII.  \\n\\nLet me know if you have any more questions! üèàüèÜ  \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hello, world (with LangChain and gemma2)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "llm = OllamaLLM(model=model_name)\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test with the ollama python library directly\n",
    "import ollama\n",
    "\n",
    "def get_completion(prompt, model=model_name):\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    response = ollama.chat(model=model, messages=messages)\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 = **2** \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion('What is 1+1?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pirate english\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is  American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text that is delimited by triple backticks into a style that is  {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Well, I\\'m quite frustrated about my blender lid flying off and splashing my kitchen walls with smoothie.  To add insult to injury, the warranty doesn\\'t cover the cost of cleaning up the mess.  I could really use some assistance right away.\" \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do the same thing using LangChain\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='gemma2:2b-instruct-q4_1', temperature=0.0, _client=<ollama._client.Client object at 0x000001FEDED2F610>, _async_client=<ollama._client.AsyncClient object at 0x000001FEDED2FE50>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOllama(model=model_name, temperature=0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_str = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# we use langchain chat prompt template to create a template that we can reuse many times\n",
    "prompt_template = ChatPromptTemplate.from_template(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that this template has two types of inputs that are generated using our\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can create customer messages that will generate the prompt using the template\n",
    "# we have to fill the input variables 'style' and 'text' with our inputs\n",
    "customer_messages = prompt_template.format_messages(style=customer_style, text=customer_email)\n",
    "customer_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\"\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On passe ce prompt g√©n√©r√© par le template au mod√®le de chat cr√©√© avant\n",
    "customer_response = chat.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Well, I\\'m quite upset about my blender lid flying off and making a mess on my kitchen walls with the smoothie.  To add insult to injury, the warranty doesn\\'t cover the cost of cleaning up. I really could use some assistance right away.\" \\n\\n\\nHere\\'s what changed:\\n\\n* **Arrr** and **matey** were replaced with more formal language like \"Well\" and \"I really could use\".\\n* The sentence structure was simplified for clarity.\\n*  The tone is less aggressive and more polite, using phrases like \"quite upset\" instead of \"fuming\". \\n\\n\\nThis version conveys the same message in a calmer and more respectful way. \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\\n```\\n\")]\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(style=service_style_pirate, text=service_reply)\n",
    "print(service_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_response = chat.invoke(service_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Ahoy, matey!  \\n\\nSo ye be wantin' a new blender, eh? Well, shiver me timbers, this here warranty don't cover no cleanin' expenses for yer kitchen. Seems ye forgot to put the lid on before givin' that ol' blender a whirl. A right mess it made, aye. \\n\\nBest of luck findin' another way to scrub up yer kitchen! Fair winds and following seas! \\n\", response_metadata={'model': 'gemma2:2b-instruct-q4_1', 'created_at': '2024-10-16T11:26:57.0914125Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3929009600, 'load_duration': 41806500, 'prompt_eval_count': 82, 'prompt_eval_duration': 526956000, 'eval_count': 94, 'eval_duration': 3358761000}, id='run-cc6876d2-86c4-4cc4-8784-76a5397c772e-0', usage_metadata={'input_tokens': 82, 'output_tokens': 94, 'total_tokens': 176})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les `parsers` LangChain permettent d'extraire et de formater la r√©ponse en sortie d'un LLM afin de la r√©utiliser si besoin.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple d'un format que nous souhaitons obtenir √† partir d'une revue d'un produit\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©√© un template de revue o√π l'on sp√©cifie au LLM exactement les informations √† extraire de la revue faite par un client\n",
    "\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# we use langchain chat prompt template to create a template that we can reuse many times\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr√©ons les messages que l'on va passer au LLM via le template\n",
    "\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOllama(model=model_name, temperature=0)\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"gift\": \"True\",\n",
      "  \"delivery_days\": \"2\",\n",
      "  \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\" \n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M√™me si la r√©ponse a l'air bien formatt√©e, on peut voir qu'il s'agit en r√©alit√© d'un long texte que l'on ne peut utiliser tel quel\n",
    "# Il nous faut donc utiliser des parsers afin d'obtenir en r√©ponse des objets directement utilisables\n",
    "type(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# d√©finir les sch√©mas pour chaque valeur souhait√©e\n",
    "gift_schema = ResponseSchema(name='gift', description='\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\"')\n",
    "\n",
    "delivery_days_schema = ResponseSchema(name='delivery_days', description='How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.')\n",
    "\n",
    "price_value_schema = ResponseSchema(name='price_value', description='Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.')\n",
    "\n",
    "# les regrouper dans une liste\n",
    "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr√©er le parser en utilisant la liste de sch√©mas cr√©√©e ci-dessus\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir du parser, on r√©cup√®re les instructions de formattage\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // \"Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\"\\n\\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\\n\\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\\n}\\n```'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec le m√™me template que pr√©c√©demment on ajoute les instructions de formattage de la r√©ponse\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on recr√©√© ensuite le template de prompt et les messages avec ces nouvelles informations\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "messages = prompt_template.format_messages(text=customer_review, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "Format the output as JSON with the following keys:\n",
      "gift\n",
      "delivery_days\n",
      "price_value\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // \"Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\"\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": \"True\",\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On peut maintenant utiliser le parser cr√©√© afin de parser la r√©ponse\n",
    "\n",
    "output_dict = output_parser.parse(res.content)\n",
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': 'True',\n",
       " 'delivery_days': '2',\n",
       " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='memory'>2. Memory</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les LLMs n'ont pas naturellement la capacit√© √† \"se rappeler\" de l'historique d'une interaction; cela est d√ª √† leurs limites en termes de tokens de contexte.  \n",
    "\n",
    "Dans cette partie, nous verrons comment am√©liorer cet aspect en gardant en m√©moire l'historique d'une interaction, en la remettant dans le prompt afin d'obtenir une conversation plus naturelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:2b-instruct-q4_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nelly\\AppData\\Local\\Temp\\ipykernel_16624\\590950356.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=model_name, temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Audrey! It's nice to meet you. My name is Bard, an AI assistant created by Google.  What can I do for you today? üòä \\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi, my name is Audrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a simple one! 1 + 1 equals 2. üòä \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='What is 1 + 1?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Audrey, it's nice to meet you too!  üòä  You told me your name was Audrey.  What can I help you with today? \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='What is my name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifier la valeur de verbose pour voir ce qui se passe sous le capot de LangChain\n",
    "\n",
    "llm = ChatOllama(model=model_name, temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Audrey\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Audrey! It's nice to meet you. My name is Bard, an AI assistant created by Google.  What can I do for you today? üòä \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi, my name is Audrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Audrey\n",
      "AI: Hi Audrey! It's nice to meet you. My name is Bard, an AI assistant created by Google.  What can I do for you today? üòä \n",
      "\n",
      "Human: What is 1 + 1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a simple one! 1 + 1 equals 2. üòä \\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='What is 1 + 1?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Audrey\n",
      "AI: Hi Audrey! It's nice to meet you. My name is Bard, an AI assistant created by Google.  What can I do for you today? üòä \n",
      "\n",
      "Human: What is 1 + 1?\n",
      "AI: That's a simple one! 1 + 1 equals 2. üòä \n",
      "\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Audrey, it's nice to meet you too!  üòä  You told me your name was Audrey.  What can I help you with today? \\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='What is my name?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'en utilisant le buffer de conversation, le mod√®le garde l'historique dans le prompt. Cela lui permet d'avoir une certaine \"m√©moire\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Audrey\n",
      "AI: Hi Audrey! It's nice to meet you. My name is Bard, an AI assistant created by Google.  What can I do for you today? üòä \n",
      "\n",
      "Human: What is 1 + 1?\n",
      "AI: That's a simple one! 1 + 1 equals 2. üòä \n",
      "\n",
      "Human: What is my name?\n",
      "AI: Audrey, it's nice to meet you too!  üòä  You told me your name was Audrey.  What can I help you with today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up?\n"
     ]
    }
   ],
   "source": [
    "# On peut √©galement ajouter manuellement des √©l√©ments au buffer\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({'input':'Hi'}, {'output':\"What's up?\"})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up?\n",
      "Human: Nothing much, justing hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({'input':'Nothing much, justing hanging'}, {'output':'Cool'})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons un autre type de buffer de conversation `qui s'√©tend sur une fen√™tre qu'on d√©finit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1 pour garder seulement une fen√™tre de contexte de 1\n",
    "# (1 occurrence pour l'utilisateur et 1 occurrence pour le mod√®le)\n",
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({'input':'Hi'}, {'output':\"What's up?\"})\n",
    "memory.save_context({'input':'Nothing much, justing hanging'}, {'output':'Cool'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Nothing much, justing hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "# avec ce buffer, le mod√®le garde bien en m√©moire 1 seul √©l√©ment pour chaque interlocuteur\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre type de buffer de conversation li√© au `nombre de tokens`.  \n",
    "Ce type de buffer est int√©ressant pour les cas o√π l'on travaille avec un mod√®le payant par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9eaf6ec7b8f4ef1850c951946eb2f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Utilisateurs\\Nelly\\Travail_Perso\\python-envs\\genai-test2\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nelly\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccfa14ae7e6440f95424799f96c9ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c85fb814264d71973a6dae1383ff10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e1b98a600d4343a6a24b5314058870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9c502b2897433187dbd25bd8bc3869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Utilisateurs\\Nelly\\Travail_Perso\\python-envs\\genai-test2\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre type de buffer de conversation se base sur un `un r√©sum√©`.  \n",
    "Ce type de buffer utilise un llm pour r√©sumer la conversation qui a eu lieu jusqu'√† pr√©sent, et ce r√©sum√© est utilis√© comme historique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},{\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='chains'>3. Chains</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La `cha√Æne`, qui correspond au bloc principal du framework LangChain, combine en g√©n√©ral le LLM avec un prompt. En utilisant un tel bloc que l'on peut r√©p√©ter, cela offre plusieurs possibilit√©s int√©ressantes dans la cr√©ation d'applications.  \n",
    "\n",
    "Une possibilit√© int√©ressante des cha√Ænes est qu'on peut l'utiliser sur plusieurs donn√©es en entr√©e simultan√©ment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:2b-instruct-q4_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\r\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Product  \\\n",
       "0       Queen Size Sheet Set   \n",
       "1     Waterproof Phone Pouch   \n",
       "2        Luxury Air Mattress   \n",
       "3             Pillows Insert   \n",
       "4  Milk Frother Handheld\\r\\n   \n",
       "\n",
       "                                              Review  \n",
       "0  I ordered a king size set. My only criticism w...  \n",
       "1  I loved the waterproof sac, although the openi...  \n",
       "2  This mattress had a small hole in the top of i...  \n",
       "3  This is the best throw pillow fillers on Amazo...  \n",
       "4   I loved this product. But they only seem to l...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer des donn√©es avec pandas\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "fichier = Path('../data/csv/chain-example.csv')\n",
    "df = pd.read_csv(fichier)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain basique\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=model_name, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template('What is the best name to describe a company that makes {product}?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nelly\\AppData\\Local\\Temp\\ipykernel_8980\\1305865249.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nelly\\AppData\\Local\\Temp\\ipykernel_8980\\3772654808.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  chain.run(product)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here are some names that could work for a Queen-size sheet set company, categorized by style:\\n\\n**Classic & Elegant:**\\n\\n* **Royal Rest** \\n* **Grand Sheets**\\n* **Serene Slumbers**\\n* **The Velvet Nest**\\n* **Queen's Comfort**\\n* **Majestic Sleep**\\n\\n**Modern & Playful:**\\n\\n* **Sheet Kings/Queens** (fun, bold)\\n* **Dreamworthy**\\n* **The Pillow Post** \\n* **Snuggle Co.**\\n* **Cloud Nine Sheets**\\n\\n**Descriptive & Direct:**\\n\\n* **Queen Size Bedding Company**\\n* **Luxury Queen Sheets**\\n* **Premium Queen Sets**\\n* **Deep Sleep Collection**\\n\\n\\n**Tips for Choosing the Right Name:**\\n\\n* **Target audience:**  Who are you selling to? Consider their age, lifestyle, and preferences. \\n* **Brand personality:** Do you want your name to be playful, luxurious, or straightforward?\\n* **Memorability:** Is the name easy to remember and pronounce?\\n* **Availability:** Check if the name is available as a domain name and social media handle.\\n\\n\\n**Beyond Words: The Importance of Logo & Branding:**\\n\\nRemember that the visual aspects of your brand are just as important. Your logo, color palette, and overall branding should reflect your chosen name to create a cohesive experience for customers. \\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = 'Queen Size Sheet Set'\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Sequential chain : combine exactement 1 input et 1 output de chaque cha√Æne\n",
    "# dans l'ordre\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template('What is the best name to describe a company that makes {product}? Give me just one name.')\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template('Write a 20 words description for the following company:{company_name}')\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains = [chain_one, chain_two],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m**Slumber Haven** \n",
      "\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mSlumber Haven provides cozy, comfortable bedding and sleep solutions for ultimate relaxation. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Queen Size Sheet Set',\n",
       " 'output': 'Slumber Haven provides cozy, comfortable bedding and sleep solutions for ultimate relaxation. \\n'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.invoke(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lorsqu'il y a plus d'un input et output, on utilise le Sequential chain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Translate the following review to English:\n",
    "    \\n\\n{review}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key='english_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Can you summarize the following review in 1 sentence:\n",
    "    \\n\\n{english_review}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key='summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"In what language is the following review:\n",
    "    \\n\\n{review}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key='language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Write a follow up response to the following summary in the specified language:\n",
    "    \\n\\nSummary: {summary}\\n\\nLanguage: {language}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key='followup_msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains = [chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables = ['review'],\n",
    "    output_variables = ['english_review', 'summary', 'followup_msg'],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review': \"Je trouve le go√ªt m√©diocre. La mousse ne tient pas, c'est bizarre. J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...\\r\\nVieux lot ou contrefa√ßon !?\",\n",
       " 'english_review': \"I find the taste mediocre. The foam doesn't hold, it's strange. I buy the same ones at the store and the taste is much better...\\n\\nIs this old stock or counterfeit!? \\n\",\n",
       " 'summary': 'This reviewer found the taste of the product subpar compared to their store-bought option and suspects potential issues with authenticity. \\n',\n",
       " 'followup_msg': '## R√©ponse √† l\\'analyse :\\n\\n**\"La saveur du produit s\\'av√®re en d√©pit des attentes, se rapprochant peu de la qualit√© d‚Äôun produit achet√© dans une √©picerie, et suspecte un potentiel probl√®me d\\'authenticit√©.\"**\\n\\n---\\n\\nLet me explain:\\n\\n* **\"This reviewer found the taste of the product subpar compared to their store-bought option\"**: This translates directly and accurately conveys the essence of the summary. \\n* **\"se rapprochant peu de la qualit√© d‚Äôun produit achet√© dans une √©picerie\"**:  We use a comparison with \"un produit achet√© dans une √©picerie\" for accuracy, but it\\'s important to acknowledge that this may not be a universally accepted term (a simpler ‚Äúd\\'une qualit√© comparable √† celle d‚Äôun produit achet√© en magasin‚Äù could work as well).\\n* **\"suspects potential issues with authenticity\"**:  This part directly addresses the suspicion regarding the product\\'s authenticity, but using the verb \"suspecte\" ensures a more neutral tone. \\n\\nThe response is straightforward and focuses on both taste and authenticity concerns. It reflects the original summary in French accurately, while adding a touch of formality. \\n'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain.invoke(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour des cas plus complexes, il est int√©ressant d'utiliser un router qui peut choisir\n",
    "# √† quel autre cha√Æne passer le r√©sultat\n",
    "\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on cr√©e des informations sur les diff√©rents prompts selon leur sujet de pr√©dilection\n",
    "# ces informations seront pass√©es au routeur pour aider dans sa d√©cision\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "\n",
    "for p in prompt_infos:\n",
    "    name = p['name']\n",
    "    prompt_template = p['prompt_template']\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valeurs par d√©faut au cas o√π le router n'a pas assez d'informations pour choisir\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template = router_template,\n",
    "    input_variables=['input'],\n",
    "    output_parser=RouterOutputParser()\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nelly\\AppData\\Local\\Temp\\ipykernel_8980\\2813769297.py:1: LangChainDeprecationWarning: Use RunnableLambda to select from multiple prompt templates. See example in API reference: https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n",
      "  chain = MultiPromptChain(\n"
     ]
    }
   ],
   "source": [
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is black body radiation?',\n",
       " 'text': \"Great question!  Blackbody radiation,  is the light emitted by an object that absorbs all incident electromagnetic radiation, regardless of frequency or angle. Think of it as the most efficient radiator possible. \\n\\nIt doesn't matter if it's a star burning in space, a hot pan on your stove, or even a tiny particle - if an object is heated to a high enough temperature, it emits blackbody radiation. \\n\\n\\n**The Key Players:**\\n* **Temperature (T):** This determines the peak wavelength of emitted light and is directly linked to the object's internal energy.\\n* **Electromagnetic Radiation:**  Blackbody radiation consists of all frequencies of electromagnetic waves - but we see its impact most clearly as visible light.\\n\\n**Why Is It Important?**\\n\\nBlackbody radiation has immense implications for:\\n* **Understanding stars**: The spectrum of light emitted by stars is a direct consequence of their temperature and composition.\\n* **The black-body law:** This law tells us how the amount of radiated energy depends on the object's temperature, making it crucial for physics equations! \\n\\n\\n**Do you have any other questions about black body radiation?**  I might be able to provide some more examples! \\n\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What is black body radiation?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'What is 2 + 2?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 2 + 2?',\n",
       " 'text': 'Let\\'s break it down:\\n\\n* **Component 1:** We are adding two numbers together (2 and 2).  \\n* **Component 2:** This involves the operation of addition, represented by the \"+\" symbol.  \\n* **Solution:** To find the solution to this problem, we add the first number to the second number (2 + 2 = 4).\\n\\n**Therefore, 2 + 2 equals 4.** \\n\\n\\nLet me know if you have any other math problems you\\'d like help with! üòÑ  \\n'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What is 2 + 2?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit bien que chaque question est bien rout√©e vers la bonne cha√Æne pour obtenir une r√©ponse √† la question pos√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='question-and-answer'>4. Question and answer</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonctionnalit√© de LangChain permet de construire des applications qui combinent les LLMs avec des documents sur lesquels ils n'ont pas √©t√© entra√Æn√©s.  \n",
    "\n",
    "Cela offre de grandes possibilit√©s car les LLMs en deviennent ainsi plus flexibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:2b-instruct-q4_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e un loader LangChain √† partir du fichier de donn√©es\n",
    "fichier = Path('../data/csv-txt/langchain/retrieval-outdoor-clothing.csv')\n",
    "loader = CSVLoader(file_path=fichier, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour nous aider √† cr√©er un index de vecteur plus facilment\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=model_name, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Utilisateurs\\Nelly\\Travail_Perso\\python-envs\\genai-test2\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "# cr√©ation de l'index\n",
    "embeddings = OllamaEmbeddings(model=model_name)\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings\n",
    "    ).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please list every shirt that have sun protection \\\n",
    "in a markdown table, and summarize each one in less than 20 words. The name \\\n",
    "attribute must mention the word 'shirt' for it to be a shirt. If a shirt does have \\\n",
    "sun protection, the description will mention something similar to 'UPF 50+'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name | Summary |\n",
       "|---|---|\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | Lightweight, wrinkle-free shirt with UPF 50+ sun protection. |  \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut refaire cet exemple en proc√©dant par √©tapes pour plus de compr√©hension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# 1. On cr√©e un loader LangChain √† partir du fichier de donn√©es\n",
    "fichier = Path('../data/csv/retrieval-outdoor-clothing.csv')\n",
    "loader = CSVLoader(file_path=fichier, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. On charge ensuite les documents √† partir du loader\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=': 0\n",
      "name: Women's Campside Oxfords\n",
      "description: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \n",
      "\n",
      "Size & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \n",
      "\n",
      "Specs: Approx. weight: 1 lb.1 oz. per pair. \n",
      "\n",
      "Construction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT¬Æ antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \n",
      "\n",
      "Questions? Please contact us for any inquiries.' metadata={'source': '..\\\\data\\\\csv\\\\retrieval-outdoor-clothing.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. On d√©coupe les √©l√©ments en \"chunks\" afin de les ins√©rer dans le vector store\n",
    "\n",
    "# PS: dans le cas actuel, les documents dont nous disposons sont individuellement\n",
    "# petits comme on peut le voir ci-dessus, donc pas besoin de les d√©couper. \n",
    "# On peut donc directement cr√©er les embeddings sur ces documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. On cr√©e les embeddings\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v√©rifier ce que fait l'embedding\n",
    "test_embed = embeddings.embed_query('Hi my name is Audrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n",
      "[-0.009170529, 0.002634769, -0.005046889, 0.009464011, 0.005023745, 0.008658773, -0.010745666, 0.0044666496, -0.0056383368, -0.0018385494, -0.0118973935, -0.011663985, -0.010653571, -0.0071752984, 0.0016451138, -0.0022024151, -0.004184519, -0.0023072183, -0.029984558, 0.004578143, 0.0019222953, -0.023707282, -0.013256571, -0.0020219907, -0.0014535794, -0.0034331062, -0.010205655, 0.00637563, -0.00927417, -0.004122926, -0.00010651525, -0.006243533, -0.010386501, -0.0026935895, 0.009630466, 0.017238561, -0.0010671235, 0.0036802157, 0.0015628256, -0.0119682755, 0.0029880374, 0.0030912233, 0.015683398, 0.0094699515, 0.003024296, -0.017271144, -0.0024301768, 0.0009829041, 0.0041862023, 0.0010873448, 0.0028968055, 0.015280737, -0.0005752692, -0.016067047, -0.018051513, 0.0035680167, 0.0077957776, 0.0015329652, 0.001622359, 0.0016017782, -0.020544639, 0.041803535, -0.0136966575, 0.0051219063, 0.008351156, -0.0017880438, -0.0059062014, -0.0001971106, -0.0012041304, 0.0104490165, 0.00840747, 0.013498973, 0.006799868, -0.009726493, 0.0014733623, -0.009091991, -0.0018798453, -0.006447387, 0.0003870945, -0.0070759286, 0.004118707, -0.01788811, -0.017117431, -0.011449206, 0.012179423, 0.0029858197, -6.673691e-05, -0.0021122438, -0.0014264393, -0.001989621, -0.0037115952, -0.0073061395, 0.0053089517, 0.0011724266, 0.0031882748, -0.018526558, 0.014633488, -0.0009257472, 0.005030806, 0.002175552, 0.0044377143, -0.0040630754, -0.008060565, -0.0018999259, -0.004604387, 0.002113823, -0.0089855185, -0.00032203257, -0.0057719434, -0.00057076145, -0.008079762, 0.00044895505, 0.013920601, 0.019761926, -0.008656988, -0.013864513, 0.00428138, 0.009594131, 0.008294683, -0.0011615633, -0.0037923898, -0.002694878, -0.008448908, -0.0038067382, 0.01218114, -0.00014541186, -0.005649053, 0.0030634927, -0.0013549, -0.0031301524, 0.008133731, 0.011293223, 0.0046589216, 0.0051155426, 0.015240598, -0.0039229295, -0.0031854718, -0.0061170147, -0.006595764, -0.0016355296, 0.16283958, 0.005576441, -0.0024728035, -0.010335386, -0.00010374739, 0.0049289986, -0.00031012832, -0.007513956, -0.002426834, 0.00030456448, -0.0004987808, -0.0018599599, -0.0029893417, -0.01430439, -0.008064563, 0.0026904393, 0.015807042, 0.0133982925, -0.016038008, 0.004570052, -0.0040523927, -0.009882103, -0.024898767, 0.0038889542, -0.006157106, 0.0002284358, -0.0015168558, 0.0047770566, -0.002302359, -0.001447555, -0.009358659, -0.0009590457, 0.008526932, -0.0028005217, 0.00026454052, 0.0034444903, -0.00046247078, -0.023907704, -0.014080263, -0.0042047654, -0.0016803559, -0.007262097, -0.0046173786, -0.004897284, 0.024794536, 0.012060184, -0.009970088, 0.0028358947, -0.0064598797, 0.004566636, -0.007995143, 0.03559537, 0.0022738338, -0.008151518, 0.0044559306, 0.004713287, -0.0057971524, 0.006450534, 0.0056353067, -0.0046381117, 0.0058934367, -0.0013628294, 0.006281666, 0.00041012673, -0.003041593, 0.007364685, 0.06148594, 0.008692477, 0.00302712, -0.0033074303, -0.011618704, 0.003950623, 1.9882173e-05, 0.0051849345, 0.0071432786, -0.0014284463, 0.0044244127, -0.012147702, 0.005721569, -0.004693167, 0.0032959126, 0.0056098057, -0.0014503732, 0.0042460887, -0.011275016, 0.005081356, 0.013467594, -0.0080488995, 0.00020320885, -0.0057007815, 0.009023357, 0.0031047743, -0.004505461, -0.001702703, 0.00453041, -0.002805396, 0.004444338, 0.0028437304, -0.0022370447, -0.0091260895, -0.010359691, -3.624091e-05, -0.005476229, -0.0047943257, -0.00153025, 0.0021608134, 0.0030533883, -0.0039673825, -0.008322213, -0.0070630643, -0.0015971127, 0.0021543433, 0.0008540865, 0.0023992267, -0.004096642, 0.0033498625, -0.003644966, 0.0022120764, -0.011717249, -0.0018672912, -0.005083847, -0.00048863207, -0.040218234, 0.004237761, -0.017951274, 0.0056192908, 0.0071845455, -0.006283334, 0.009206938, 0.0012110124, -0.010286936, 0.0027671233, 0.005422398, -0.0037525834, 0.014509631, -0.0060762772, 0.00039135537, 0.012029605, 0.0072943335, 0.0008426481, 0.010037441, 0.004740134, 0.0034998741, 0.0024518052, 0.017006896, 0.00035822086, 0.0026378823, 0.0014933599, -0.01044281, 0.0106926495, 0.0046360986, -0.006714502, -0.009425819, 0.010267867, 0.0028380777, 0.00071697903, 0.0064186966, -0.0036684726, 0.009709842, 0.0057402463, -0.01577286, 0.0063234847, -0.010525482, 0.0054408563, -0.0042693354, 0.0029312463, -0.012556681, -0.012022843, 0.004147216, 0.0034875886, -0.005495521, 0.013987445, -0.0072885323, -0.009918051, 0.004385999, -0.009072269, -0.0032120594, 0.0059031807, -0.007593193, 0.0024883014, -0.0038156703, -0.019841362, 0.002174774, 0.0026073672, 0.0006275314, -0.008445669, -0.00283765, -6.186317e-05, -0.0026442995, 0.012040529, 0.003483144, -0.0060554896, -0.012325468, 0.0033865867, 0.018051796, 0.0074563567, -0.016847495, -0.009871448, 0.009969093, -0.019107038, 0.004932945, -0.0035489472, -0.0023178358, -0.0118549485, -0.00614413, -0.0013012501, -0.00805338, -0.013040732, 0.0021453532, 0.0011386711, 0.01176262, 0.0023909267, 0.0021632232, -0.004487767, -0.0011687849, 0.019949885, 0.00011064925, -0.014221978, -0.004437844, 0.0002639141, 0.0003341109, 0.010988518, 0.006135667, -0.013396631, 0.012644742, 0.0014543347, -0.013799654, -0.023934461, -0.013179192, -0.0028623766, 0.0027144572, -0.009533981, -0.0040553277, 0.0047011073, 0.0050955126, -0.00058993604, -0.008276673, 0.00051903614, -0.004231599, 0.0024547507, 0.0011339105, 0.009076377, 0.007949228, -0.0013409638, 0.0051572924, -0.0031058213, -0.012968753, -0.0062760324, 0.0034097314, -0.0055025215, -0.01868796, -0.0007033207, 0.0008244463, 0.0038983563, 0.012985165, -0.009937225, 0.00015806402, 0.000171753, -0.0062660296, -0.002562436, -0.010053895, 0.008430848, -0.009140056, 0.011583777, 0.007827283, -0.013927021, -0.010932178, -0.0022091863, -0.00908222, -0.018744139, 0.0025160885, -0.0033963895, -0.0060236603, 0.0021105537, -0.012799139, -0.009457924, -0.0054468005, 0.0031313163, -0.002227166, 0.012821551, 0.01122925, 0.004242064, 0.01896739, -0.015659036, 0.00411468, 0.014797604, -0.0016239941, 0.0034408998, -0.011982412, 0.0020507628, 0.019292202, 0.007774527, -0.0038594734, -0.00912169, -0.010698762, 0.0041552954, -0.0019863036, 0.0039077313, -0.00041100752, -0.00094777555, -0.015287206, -0.015711559, 0.0060482305, 0.006608577, 0.004180021, 0.0063372054, 0.0052954042, 0.0034754544, -0.0028009221, 0.0027970606, 0.0037570817, 0.00042502608, -0.0017625778, 0.0037179068, 0.00596117, 0.0020707222, -0.012953327, 0.0072520375, -0.008084187, 0.0036786988, 0.0017793461, 0.008810017, -0.0069479765, 0.000493263, -0.0050499607, -0.00079240993, -0.0056283004, -0.00647481, -0.0060083405, 0.003910944, 0.0077890395, 0.012401874, 0.011431677, 0.006325969, -0.004647758, -0.0013927773, 0.015427728, -0.00014076, -0.002892662, -0.005072247, -0.004823922, 0.015592052, 0.008682591, -0.008468134, -0.003116863, 0.0037458825, -0.011897363, 0.00805532, -0.002165164, 0.0009678454, -0.0067335614, -0.0033057262, -0.0017895127, -0.002916631, 0.0051439414, 0.005717559, -0.0007729717, -0.0061109765, -0.0049781, 0.0017593009, -0.014963591, 0.011653773, 0.003691583, -0.00041088864, -0.008038735, 0.00072301563, -0.026837194, -0.004312102, 0.0089498, -0.0045073917, -0.0068263477, -0.010792534, -0.0032392964, -0.013650405, 0.0043148044, 0.0042122765, 0.007039853, -0.007439814, 0.0035007335, -0.007188426, -0.0051459842, 0.00851587, -0.009851586, 0.00905725, 0.0050672833, 0.010349252, 0.0029208956, 0.005965178, 0.0013045279, 0.001822033, -0.0045589274, -0.0053591304, 0.0035202547, -0.0033091472, -0.0016671934, -0.001759853, 0.008422575, -0.07422806, 0.0027240817, -0.007726265, 0.0062520667, 0.009061691, 0.01246508, -0.0005410631, -0.0025984722, 0.018084183, -0.0038516151, -0.0063637593, 0.0156144835, -0.0009209051, -0.0081038475, 0.0013016129, 0.0021697297, -0.008442474, 0.0056155194, 0.0031025612, -0.0018480592, -0.012847477, -0.026834741, 0.0033152748, -0.0029690105, -0.025751589, -0.005999226, 0.006614154, 0.01404171, 0.013369476, -0.006998561, -0.003737723, 0.0052141, -0.016608832, 0.017985806, 0.010672757, -0.005899322, -0.00019982402, -0.0048450544, 0.0054976004, 0.003690885, -0.012563831, -0.0031315265, -0.0054478943, 0.05926863, 0.004034047, -0.0030029265, -0.0035343883, 0.029624313, 0.0077904332, 0.028473234, 0.005864438, 0.0013308555, -0.00811289, 0.01572672, 0.00062022236, -0.010050884, 0.27153978, 0.005343585, -8.7028035e-05, -0.005713455, -0.006526717, 0.01322217, -0.0042724907, 0.011751523, -0.008816671, -0.009611049, 0.00019510441, -0.009577215, -0.0035987599, 0.01055162, -0.0015705316, -0.00070581713, 0.00067472964, 0.0036371953, 0.007145685, 0.00066497666, 0.0028898122, 0.006502458, -0.0013283981, -0.007101061, 0.0024032623, -0.0020298765, -0.0058056936, 0.000644796, -0.006459166, 0.0009403858, -2.645779e-05, 0.007917388, -0.0089513585, -0.008393603, -0.00783406, -0.010875867, -0.011734865, 0.0061244844, 0.0043915166, -0.008180583, 0.0024391185, -0.00046463733, -0.005065994, -0.0051728184, 0.008268085, 0.015987625, -0.003681427, 0.0074671404, -0.008778253, 0.00851918, 0.004793504, -0.004949135, 0.0019822945, 0.0019449285, -0.005585168, -0.011781704, 0.013153815, 0.040400833, 0.012963067, -0.002302831, -0.0018899498, 0.0053036176, -0.0056995596, 0.0008252562, -0.008457117, 0.0043175025, -0.0025119826, -0.0124999555, -0.0015773575, -0.00055024377, -0.00010273845, 0.0030284375, -0.00042585967, -0.0031688663, 0.0019600063, -0.008360167, -0.014839831, 0.016363636, 0.005297016, -0.01138927, -0.0006993725, 0.0030275898, -0.0018717119, 0.0008931439, -0.0010177734, 0.0069213114, 3.2715798e-05, -0.0046209865, -0.0043166876, -7.3675133e-06, 0.007271123, -0.0076784464, -0.0006219562, 0.010858283, 0.0032245894, -0.0011726887, -0.16939893, -0.0035033508, -0.0023316173, -0.0050578723, -0.0019219824, 0.0033845126, -0.008254431, 0.002937607, -0.0023265837, 0.0050454848, -0.008193409, -0.0035670267, 0.0014180858, 0.0017999979, 0.017255597, -0.017770024, -0.0058319457, -0.0032206154, -0.0108164335, 0.0071503115, -0.0044114245, -0.0053351833, -0.0057176203, 0.012474361, -0.011809589, -0.044372685, -0.017127056, 0.011804425, 0.00064980186, -0.0041367575, 0.01254659, 0.0058311927, 0.001847657, -0.0057545463, 0.002958377, -0.006418618, 0.011744652, -0.0029083388, 0.013579421, 0.0070008356, -1.0703148e-05, -0.0013029287, 0.0016587342, 0.0074403114, -0.0018486836, -0.005973487, 0.0059596584, 0.010177955, -0.0053359116, -0.0005480742, 0.0019798104, -0.0034374385, 0.0027695184, -0.005747402, -0.007773379, -0.0028771905, -0.0058418857, 0.0058201486, -0.011283386, -0.00096714095, -0.0005097884, -0.004229749, 0.0020973063, 0.0019232174, -0.013248, 0.0045483853, 0.009535427, 0.010043683, 0.012366441, 0.007510973, -0.006307679, 0.0015578388, 0.0011914005, 0.012191924, -0.014117341, 0.00025062484, -0.004773353, 0.008049895, 0.008104041, 0.0060538203, 0.0025804166, 0.0032086628, 0.003019723, -0.007062297, 0.004084046, -0.0013629061, -0.00246006, 0.015158424, 0.014147062, -0.048074953, -0.0009809877, -0.0034359302, -0.0036123786, 0.00486593, 0.0038679596, 0.0073140366, -0.0046363343, 0.0022575038, -0.0010506441, 0.002472765, 0.004299918, -0.013396712, -0.014332199, 0.016449101, -0.0002518186, -0.013151086, 0.0010856215, -0.0051528504, 0.004271642, 0.008928024, -0.0008689728, 0.0016620391, -0.0035947845, -0.00045638005, -0.005592628, -0.0071797, -0.0041539813, 0.0043766135, -0.0027711072, -0.0016863837, -0.013265782, 0.008770031, 0.0007424814, 0.0049439687, 0.012940912, -0.0057318234, 0.017705873, 0.44358525, -0.0021667744, -0.0055035455, 0.008463709, 0.0017432743, 0.004886916, -0.0023427533, 0.0019160676, -0.011681926, 0.0029505165, -0.0025750678, -0.012446283, 0.0003887251, 0.011116343, -0.0043259594, -0.006259385, -0.004411483, 0.42678094, -0.0071540214, -0.006232231, 0.0027996465, -0.0005165228, 0.005570589, -0.004906268, 0.0012475562, 0.0014304755, 0.019205261, -0.0010861864, 0.0018138424, -0.0053358627, -0.07627994, 0.00844364, -0.00066705357, 0.008984, -0.0020625636, -0.00012775966, 0.0035171066, 0.0035504517, -0.03198976, -0.0059215333, 0.0041279476, -0.015680015, -0.0015883163, -0.008051429, -0.001078616, -0.0076921475, 0.001931026, 0.0028466717, -0.019589206, 0.0008277096, 0.008022601, -0.0026328904, 0.0054503735, -0.005200093, 0.0030400353, -0.012216881, -0.0067967144, -0.017088015, -0.0012162477, -0.0051535857, 0.010550864, -0.0005228798, 0.005201051, -0.010297349, 0.00925106, 0.0111440215, 0.0058506713, -0.0047789277, 0.014753636, -0.0075505916, 0.005027536, -0.00079292536, -0.0031994015, -0.015841553, -0.009754652, -0.0005010293, -0.003256374, 0.0041250186, 0.0012665016, 0.007472006, 0.00026777992, -0.008294229, -0.006835262, -0.008853557, -0.010570844, 0.011433579, 0.0015176578, 0.014890942, 0.005099295, -0.014803466, 0.014793572, -0.009700491, -0.009840536, -0.017584803, 0.0046101213, 0.0058277957, -0.006145444, 0.00065756484, 0.015604559, -0.006977723, 0.0019342449, -0.0011891895, -0.0076381247, -0.021616664, 0.005042358, -0.0115272645, 0.0033315055, -0.01095548, -0.0011782382, 0.0031783616, -0.0038909777, 0.011469439, 0.007341908, 0.004259579, -0.01325891, 0.0056817024, 0.0071901777, -0.008552998, -0.0053121927, 0.01828838, -0.011171609, -0.0020583135, 0.0043848916, 0.0050540827, 0.012175253, -0.007197444, -0.004590973, 0.019383993, 0.020509912, 0.006098833, -0.00037175787, 0.005851052, 0.0018358249, 0.0009367053, 0.0032634027, 0.0014529927, 0.010374085, -0.003713352, -0.024734776, 0.0034530172, -0.013389024, -0.0014720634, 0.0018287953, -0.00072182296, -0.00046071634, -0.0122269485, 0.0061771525, 0.0008281218, -0.007076179, 0.008136496, -0.005166814, 0.008654975, 0.0026558796, -0.020211961, -0.0184934, -0.008687181, -0.009488589, -0.0059869154, 0.009575301, -0.00070291257, -0.25110456, -0.0059904284, -0.0058262562, -0.003633159, 0.0026500467, -0.006438151, -0.010583596, 0.004367195, -0.0027371196, -0.010018588, 0.00046424457, 0.012927383, -0.0057773436, 0.00928039, -0.007968339, 7.4443655e-05, -0.010323635, 0.011358007, 0.009701035, -0.010885231, -0.008916342, 0.008385522, 0.0018050013, 0.0017125029, 0.0036521335, 0.008549081, 0.0023562508, -0.00026214193, -0.0026778076, 0.007060874, -0.0059324866, 0.012104856, -0.0047331476, 0.009428707, -0.0005684999, -0.0016405329, 0.011232043, -0.0047020735, -0.0027556135, -0.009888487, 0.004608881, 0.00022862466, 0.0058935876, 0.0011214534, 0.008207059, -0.004573269, 0.0003634456, 0.0036021837, 0.012907003, 0.0014584293, 0.00676052, -0.0012265918, 0.0031803532, -0.00028201178, 0.004748038, -0.002432051, -0.0015249009, -0.0015390856, 0.0069547677, -0.004305941, 0.006362673, 0.006966169, -0.012080319, 0.0022945204, 0.008060793, -0.0076153707, 0.001191127, 0.01092428, -0.010746682, -0.0017371528, -0.0022840537, 0.005428174, -0.00022144729, 0.005596785, 0.010761591, 6.3789885e-05, 0.01301516, 0.0013119928, 0.00070794614, 0.007764324, 0.00804979, 0.0038897507, 0.01174106, 0.009989917, -0.011655719, 0.0023329917, -0.011512068, -0.0078108534, 0.0065358393, -0.0070483275, -0.005330464, -0.0015470777, -0.005324678, 0.0037417475, 0.005582882, 0.0033467151, -0.0036252595, -0.016911801, 0.00015726284, 0.0061601168, -0.014900774, 0.0078031668, -0.0021864942, 0.002743347, -0.0007432241, 0.0051058754, -0.001392883, -0.0125076845, -0.010300333, 0.0064004445, -0.009192776, 0.015586051, -0.0037119607, 0.0023200002, 0.0046276464, 0.0026050266, -0.001024491, 0.00030673743, -0.012474723, -0.011078107, 0.008443953, -0.007771488, -0.0017015513, 0.0065944567, 0.0019741778, 0.05471885, 0.017460387, 0.005558358, 0.0030018822, 0.00762174, 0.009072291, 0.0038392593, 0.0001509788, 0.002995483, -0.00017365583, 0.010048589, -0.00153808, -0.011620256, -0.0003475272, 0.018277869, 0.022632703, -0.012130404, -0.0044569243, 0.0019635628, 0.0046089175, 0.013961332, 0.005502852, -0.012031674, -0.00023255589, 0.008318357, -0.001657018, 0.0051057152, -0.0015652098, 0.0013991988, -0.0133487675, 0.0059114946, 0.0010691052, -0.0069830804, -0.003747315, 0.0030690564, -0.0024676542, 0.00039190095, 0.00050454127, -0.0014545063, 0.006881082, 0.0061758254, -0.12015145, 0.0030870677, 0.0014243295, 0.0023243406, -0.0026196963, 0.0017767171, 0.004160731, 0.004145718, -0.0054591885, 0.005903204, 0.002881137, 0.005682491, 0.0066217924, 0.0048130117, 0.0015379286, -0.0010228519, 0.0042700605, -0.007580066, 0.0069391527, 0.008518802, 0.003048334, -0.012152944, -0.007515303, 0.0015624876, -0.004397674, -0.006223002, 0.00064718875, -0.010246641, 0.0042430595, -0.0045585684, -0.015890159, 0.32108313, -0.00849223, -0.012529464, -7.515878e-05, 0.0039107543, -0.0018035983, -0.0049504736, -0.0038862685, 0.007773035, -0.0053305095, 0.0004916206, 0.0024883067, 0.0035666188, 0.0063400096, 0.005703334, -0.009099361, -0.0029393258, 0.011742438, -0.0018898753, 0.008947772, 0.0052211573, 0.005229573, 0.0049257353, -0.0024075895, 0.0024918765, -0.005770356, 0.0032162347, 0.0136698345, -0.0045182197, -0.01460964, 0.0027693002, -0.006454916, -0.014778619, -0.021221083, 0.014882385, 0.0029463542, 0.003999428, 0.0015581858, 0.0004687243, 0.007655622, -0.0047042444, -0.005553485, -0.008476598, 0.004616926, -0.0029970943, 0.0028491432, 0.002784517, 0.008419748, 0.014457999, -0.011246335, 0.01477033, -0.006471833, -0.015879381, -0.0006617499, -0.07459524, 0.0047280476, 0.0030635276, 0.002388006, 0.0018203844, 0.0036047366, -0.0035981683, -0.0029815268, -0.0069634146, 0.0061094463, -0.0038941267, -0.0002976179, 0.0011104684, -0.0582526, -0.010275751, 0.0035490955, -0.006293353, 9.181383e-05, 0.011965495, 0.005277742, 0.0057263956, 0.02998498, -0.011802344, -0.013435315, 0.005591443, 0.0051573543, -0.010694431, -0.00031951652, 0.008288218, 0.0024788473, -0.01092462, 0.0020057573, -0.0053526694, -0.001988382, -0.0003172446, -0.010053651, -0.0068857954, 0.0044172043, 0.010371364, -0.0043794084, -0.0056417678, -0.0055299187, 0.009692676, 0.005842494, 0.00073259027, -0.0045686155, 0.005519836, 0.006609925, -0.0037009036, -0.011138636, 0.0053075994, 0.002834647, 0.003860723, 0.0007660259, -0.0046259738, -0.002074929, 0.005960736, -0.004350862, 0.023332296, -0.004685778, 0.0012854974, 0.008299149, 0.013446486, -0.009057042, 0.0051731067, -0.022344293, 0.01099944, 0.0027101713, 0.0123930145, -0.019497093, 0.008289713, 0.003216211, -0.0021347678, -0.010638546, 0.0046933177, 0.00048148827, 0.00051736995, -0.20346095, 0.011035486, -0.0077978843, -0.0029887029, 0.00012011225, 0.00021109078, -0.004184357, -0.00632009, -0.0032316612, 0.0044601657, 4.678967e-05, -0.009925174, -0.0027460076, -0.007230565, 0.0011184058, -0.0002967973, -0.0011713769, 0.0062065586, 0.016026556, 0.0007544365, -0.00080259086, 1.44833875e-05, 0.0020410842, -0.008021008, -0.010555275, -0.0007176142, -0.0067246454, -0.0025185714, -0.002467417, -0.0074253213, -0.0017211101, -0.013299989, 0.0007670913, -0.004433484, -0.0072241616, 0.004240716, -0.003260198, 2.3103932e-05, 0.010582093, 0.010462421, 0.0007229948, -0.010642177, -0.0050700475, -0.006638209, 0.0021451502, -0.07609201, -0.0016638342, 0.0055446005, 0.0076155546, -0.004424805, -0.010199394, -0.0034118535, -0.0048971, -0.006487596, -0.0031012478, -0.0017655393, -0.0024727522, 0.0015036433, 0.019035583, -0.0018405998, 0.01229595, -0.0075539304, 0.003929885, 0.0051501063, -0.032718346, -0.0034852864, -8.3613355e-05, -0.0030984213, -0.0100020375, -0.005339371, -0.0015974651, 0.009146106, -0.009188652, -0.0059711966, 0.0063235033, -0.003965288, -0.01980098, 0.012539586, 0.0069147674, 0.007180648, 0.0019684224, 0.009966825, -0.010018474, 0.0020737313, -0.008432813, -0.0042507914, 0.0014127984, 0.009474724, -0.023850849, -0.036366343, -0.0037978692, 0.0011670352, -0.011744453, 0.00625359, -0.0010735549, 0.01563692, 0.0029637283, -0.0107218055, -0.008463527, 0.005128379, 0.011654919, -0.014877637, 0.004492167, 0.0008024332, 0.008542059, 0.0071472293, 0.005403398, 0.0719404, 0.02897463, -0.012941999, -0.0072099026, -0.0028708805, -0.0013301778, -0.004291698, 0.01405693, 0.01311611, 0.00060734886, 0.00017568977, 0.00026427896, 0.006994744, -0.0093272375, 0.004659451, -0.013378716, 0.0011228714, -0.0068822387, 0.0032697623, 0.00495542, -0.0065401695, -0.0029945192, -0.0012497073, -0.0018587237, -4.8546644e-05, -0.0024153779, 0.009894714, 0.0035730689, 0.004615069, -0.013752404, -0.005287531, 0.008265479, 0.0017925298, -0.014625989, 0.0011034599, -0.0013175186, -0.0018550799, 0.0045185364, 0.004786856, 0.0022066182, -0.0008157743, 0.014016017, -0.00028504521, 6.4498327e-06, 0.0013034388, -0.0064153606, 0.0011187721, 0.13846232, -0.00698343, -0.0057755033, 0.003214555, -0.0056593637, 0.0010805647, -0.0016151639, 0.0011052247, -0.0020819867, 0.004033673, 0.0013945239, -0.0030664788, 0.006878233, 0.013089636, -0.0015322426, 0.01397535, 0.00018358234, 0.007336347, 0.0077680903, 0.0067956876, -0.0021799004, 0.0029462709, -0.0040071593, -0.003070363, 0.008678061, -0.015008906, 0.017141012, -0.006750544, -0.0002903207, -0.0017851263, -0.04114199, 0.013434888, 0.0033757212, 0.0043773483, -0.0070322505, 0.013199891, 0.011217816, 0.004141862, 0.0026420818, -0.007072924, -0.012603231, 0.009031083, 0.00091537705, -0.0012558475, -0.007972544, 0.011142232, 0.0074790493, 0.006940037, -0.013452191, -0.0037269883, 0.014076431, -0.0058281026, -0.0028118514, -0.0023073032, -0.0018260741, 0.005375051, 0.12876713, -0.0089903595, -0.006213797, -0.015734164, 0.0023115948, 0.0010087204, -0.0035582602, 0.0026889942, -0.0030194307, -0.013544397, -0.0020403476, -0.004658809, -0.0035222997, 0.011650318, -0.0004525621, -0.014868965, 0.012228103, -0.006197366, -0.0072809244, 0.011834459, -0.009618937, -0.16666232, -0.0033330042, -0.006682994, -0.009323716, 0.0065351166, -0.005609621, -0.0040956894, 0.008842404, 0.0050246683, -0.00240337, -0.010457425, 0.008683119, 0.0054435716, 0.012848748, -0.0035287412, -0.0025386242, -0.0005054267, -0.0001840029, -0.005935955, 0.008798754, 0.008195489, -0.00045831784, 0.00519328, 0.005276055, -0.008208022, -0.006369294, -0.006487093, -0.0074641868, -0.0005463172, -0.0053040446, 0.0023867919, -0.0031003414, 0.007577347, -0.010065592, 0.0021254052, 0.00458072, -0.004599311, 0.012462294, -0.018961322, -0.001990346, 0.0027200575, -0.0017141711, -0.0020256874, 0.0032237596, 0.012749558, -0.012452575, 0.003322993, 0.0047934325, -0.003781439, -0.0027503918, -0.0017991534, -0.0056346017, -0.0021311285, -0.0046434486, -0.00040464365, -0.0038351614, 0.0027759261, -0.002512619, -0.00094404834, -0.0020223598, 0.00076112745, 0.0064562084, 0.0019796307, 0.0034986227, -0.00020765737, 0.0075787613, 0.0014387248, -0.0065187598, 0.0035436237, 0.0024470252, 0.00053060864, -0.005172671, 0.005701049, 0.0107238535, 0.010379247, -0.018051034, 0.0023528019, 0.0013673619, -0.0028925098, 0.006401514, 0.0029306314, -0.01025568, -0.007875949, -0.0025067118, -0.020337593, 0.004597052, 0.0034624145, 0.004327403, 0.0023183655, -0.0020072835, -0.005189489, -0.0075677633, -0.00283055, 0.0014809261, -0.005581589, 0.0066513363, -0.009305493, -0.014363155, 0.010684459, -0.002110219, 0.0008317578, 0.0007230354, 9.443915e-05, -0.0020167683, 0.0016181276, -0.00420961, 0.0049313554, 0.012830356, 0.011647595, -0.0027256315, 0.036062542, -0.0023650327, -0.0017564865, -0.0051098717, -0.010539982, -0.009836341, 0.014045139, -0.0039459234, -0.00415344, -0.005851765, -0.005526993, -0.0071243322, -0.010498003, 0.016878653, 0.0073438347, 0.002162926, 0.0006949542, 0.002871082, 0.001767542, 0.005656569, -0.0007882083, -0.004265639, 0.0069128163, -0.0035283254, 0.004235646, -0.00032867788, -0.011053015, 0.0033754208, 0.003653344, 0.0062569957, -0.0044097006, -0.00019539121, 0.0059949947, 0.015287023, 0.0134168975, -0.0022279457, -0.0005744074, 0.00030482892, -0.0034053735, -0.0050685173, 0.0023645144, 0.011914463, -0.015467109, -0.024114495, -0.016298398, 0.0036275315, -0.009120332, -0.001326817, 0.0024023242, -0.0005808887, 0.0034058488, -0.0037606123, 0.0013829194, 0.0012660112, 0.0130793, -0.0050546047, 0.00018639948, -0.013030482, -0.00032306396, 0.020255152, 0.0020094917, -0.0008521005, -0.00784873, 0.001749933, -0.006934812, -0.01591309, -0.0012294856, 0.003099924, 0.002025802, -0.011808957, 0.0034712858, -0.00034976096, 0.0064255837, -0.0037976808, -0.0019042015, -0.005773975, -0.010008277, -0.00018258592, 1.8804067e-05, -0.0031918215, 0.039448958, -0.012807274, -0.006689303, 0.005649692, -0.013047363, 0.010077735, -0.0074720485, 0.0055683376, -0.0061632614, -0.006370885, 0.0032359231, -0.004883273, 0.0055701844, -0.0028553659, -0.0054725334, 0.0025127402, 0.019232787, -0.005484826, -0.012151012, -0.009696308, -0.0032401779, -0.005239637, 0.0053315223, -0.0034261069, -0.011181324, -0.005857458, -0.0061181635, 0.0014778919, -0.011425079, 0.013515831, -0.0022293243, -0.006081711, 0.0003198088, 0.0014134685, -0.007963438, -0.017056031, 0.0019818689, -0.013987025, -0.0051247217, 0.004832348, 0.001657861, -0.006772935, -0.0047932104, -0.0077340114, -0.0014776156, 0.011274115, 0.0060637807, 0.006156988, -0.0067609996, -0.0057933764, 0.010884927, -0.0072208983, 0.0016619534, 0.010555912, -0.0009349903, -0.0039718677, -0.0055220514, -0.0059158183, -0.0068422807, -0.0018596043, 0.00830349, -0.0023392157, -0.005137272, -0.0009313391, -0.0051857634, -0.0005928588, 0.0036538702, -0.0060316217, -0.0073931664, 0.0027814386, 0.016341707, -0.012385325, 0.0029133388, 0.0014067433, -0.0009490161, -0.0043665, -0.0014950666, -0.007951743, -0.002181475, 0.0048702196, -0.006960633, -0.00093643705, 0.03155247, 0.0054213437, 0.0058322903, -0.014980617, 0.0063966853, 0.013239626, -0.0059470464, -0.005714842, -0.0036134368, 0.0065219225, -0.0021408882, 0.005719265, 0.006737696, 0.010394239, 0.0027412623, 0.018918606, -0.0016879478, 0.0014807931, 0.0017813585, -0.0019417275, -0.0042112214, 0.00076726533, 0.004860158, -0.0018662274, 0.005832956, 0.00080827746, -0.018738646, -0.0033448478, 0.0002865043, 0.007819164, -0.0044193147, -0.0074119386, -0.0034570603, 0.0002699638, 0.010900356, -0.0051928125, -0.005459523, -0.001658048, 0.0065453835, 0.0040984456, 0.0020749369, -0.0013165743, -0.0026107682, -0.006315432, -0.0005707814, 0.00790964, 0.013594672, -0.007772958, -0.008755133, 0.0005544396, 0.002014392, 0.0106567675, 0.0078111542, 0.009589642, 0.005760778, -0.00027062895, -0.0064498023, -0.0028898695, -0.0040585035, -0.013623112, 0.009938887, 0.0076529393, 0.007968625, 0.011856381, 0.0049534393, -0.031030733, 0.004155762, -0.0011381371, -0.0071824165, -0.0102029685, 0.0025371304, 0.0010492828, 0.008529947, 0.0016591629, -0.009087223, -0.0066798665, 0.0030698725, 0.003436456, 0.00233166, 0.008972679, 0.0025058757, -0.012165344, -0.001065875, -0.0038920867, 0.015480305, 0.008106494, 0.0076574525, 0.0027929149, -0.0031238617, -0.000726753, -0.014319894, 0.007197688, -0.0017862013, -0.0045836484, -0.0022981598, 0.015609152, -0.010957314, 0.0014981552, -0.014659234, -0.002944436, 0.009709465, 0.0005370836, 0.0048909253, 0.00993914, 0.0031173662, 0.014502228, 0.0020298823, 0.00076745177, 0.0015055006, -0.013556696, 0.010027302, 0.010538994, 0.009140034, 0.0011654474, -0.0021351476, 0.0029840074, 0.005335587, 0.010905148, 0.00687219, 0.004806088, 8.518022e-07, -0.0060762647, 0.0025707965, -0.0039737113, 0.010714014, -0.0008847472, -0.004279862, -0.000168408, 0.00080901274, -0.0020468594, -0.0057643033, 0.00394744, -0.008388521, 0.005724326, 0.0067208568, 0.0021251761, 0.006910058, 0.007211062, -0.006509288, 0.01593055, -0.0010259119, 0.00020587018, 0.0058415523, 0.009274409, 0.007855349, -0.007038546, 0.0020469648, 0.006137545, -0.0021936384, 0.013504441, 0.006590426, -0.008189726, -0.0002245386, -0.0043005617, 0.006389839, -0.003384231, 0.041827496, 0.005654184, 0.004098585, -0.001766907, -0.0012927623, -0.006684446, 0.00124025, -0.009251273, 0.0069577126, 0.00049107574, -0.0009273953, -0.0012109149, 0.0038279782, 0.009094808, -0.011955728, 0.012919119, 0.0020753401, 0.0032213805, -0.0061365473, -0.003245403, -0.010113007, -1.5161115e-05, 0.007325493, -0.0072804964, -0.0042180945, 0.0018126265, 0.015142324, 0.00770303, -0.0043445975, -0.017885383, -0.0068711545, -0.0017997349, -0.0021318595, 0.0013944039, -0.014835179, -0.0021033746, 0.00751028, -0.007875412, 0.010833111, -0.00021272583, -0.0154023385, -0.005662525, 0.002281658, 0.003568147, 0.004052437, -0.011329117, 0.0003271803, 0.0077566584, 0.00758469, 0.005303013, -0.0030572012, -0.00652766, -0.0041465354, 0.0057262466, -0.0046879193, 0.0065438254, 0.013664934, -0.0097964555, -0.0038029375, 0.0036992508, 0.01052445, -0.0016584691, 0.0014771959, 0.0016967208, 0.0016074051, -0.019043298, -0.013268892, -0.0070312894, 0.0018422705, 0.006289288, -0.0021548057, -0.0016827085, -0.01795844, -0.0059063155, 0.011248597, 0.004631542, 0.0068295817, 0.0036722403, -0.003996845, 0.0067035547, 0.005024479, 0.0040721884, -0.0060436474, -0.007690779, -0.0035103026, 0.00680207, 0.0028039569, -0.004758549, -0.00072876195, -0.007390075, -0.00027333552, 0.009723921, 0.0074232584, 0.0033567974, -0.0038831593, -0.006821889, -0.018247439, -0.019255165, -0.006595037, 0.00021464365, 0.015273946, 0.0039030781, 0.0011996664, 0.012287354, 0.0029680603, 0.0016860049, 0.0042729783, 0.011341573, -0.00028835156, -0.003153452, -0.0039096824, 0.0016044329, 0.009489417, 0.0074512684, 0.00628676, -0.0050654984, -0.0154228825, -0.009621689, 0.0020975717, 0.003458834, 0.007054892, -0.042865388, -0.0013556728, -0.003047466, -0.0008264961, 0.005660739, 0.0043382263, -0.024312686, -0.0006300585, 0.0019338131, 0.00038424862, -0.017045515, -0.0024948092, -0.00601059, 0.0041293935, 0.0011865366, 0.015528603, 0.01188655, -0.005961665, 0.004144064, -0.016032236, 0.0044860966, -0.0016082308, -0.012483434, -0.0025551917, -0.017774154, -0.00069034944, -0.0032326165, 0.0025535321, -0.0069304886, 0.009528459, -0.0027208829, -0.017314637, -4.6252535e-05, -0.0066812495, -0.005386707, 0.010169677, 0.0035234701, -0.007972345, -0.00885464, -6.442898e-05, 0.0041789985, 0.0014871977, -0.00034143252, -0.015344253, -0.013237325, 0.0008593151, -0.0016539237, -0.01713739, -0.0033749607, -0.008004209, -0.00071456144, -0.0016815657, 0.004126571, 0.014768139, -0.004933039, 8.2980325e-05, -0.0053900452, -0.002223315, 0.014791341, -0.013438437, 0.006189723, -0.0023025929, 0.0073892376, 0.010082829, 0.0021659608, -0.005786375, -0.0010981256, -0.0126826065, 0.012541789, -0.006670038, -0.01346452, -0.0017738612, -0.015800755, -0.0053261304, -0.01631725, -0.002045787, 0.004866097, -0.004637226, 0.0023789385, 0.00419658, -0.00039755568, 0.009879472, 0.0013788876, -0.0058824806, 0.006157749, 0.001371209, -0.0051746625, 0.0020341205, 0.000924631, 0.007938226, 0.0007634637, -0.0002570526, -0.010262987, -0.00059308816, 0.004395246, -0.0038132314, 0.0045102225, 0.0017606121, -0.011444875, 0.0020461176, 0.0010758097, 0.0007227281, 0.0007299823, -0.008501775, -0.005364943, -0.0052735237, 0.0071943505, 0.0017020367, 0.0020498976, -0.00011456141, -0.001465981, 0.0007823187, 0.0043087094, -0.0011671238, 0.013083129, -0.00891962, 0.0032755511, -0.008737895, 0.007997999, -0.003991041, 0.0021536413, 0.00085778994, -0.003881629, 0.0013926175, 0.000829117, 0.006288806, -0.008271835, 0.006383404, -0.0029082869, -0.020189518, -0.00037826828, -0.0058915857, 0.01902519, 0.011186831, 0.001994669, 0.0061175516, -0.005329121, -0.0050930306, -0.020180358, -0.0068685203, 0.00012085691, 0.006781958, -0.0050079785, -0.00038955008, 0.0041252146, 0.011233987, -0.011084728, -0.0007042758, 0.007307696, -0.0008302192, 0.030348042, 0.0048949844, -0.011641643, -0.0057043307, 0.0019013734, 0.0063034957, 0.02110603, 0.014310972, 0.01786036, -0.0015837867, -0.033234566, 0.0048219417, -0.0045783604, 0.0073308395, -0.004653551, 0.013657988, 0.004778876, 0.017198844, 0.0013896698, 0.0074620694, 0.0018504013, 0.00230144, 0.0003440384, -0.001609389, -0.004203174, 0.005566494, 4.3269796e-05, 0.009264345, -0.004733267, -0.005517073, 0.002208628, 0.0035454778, 0.008636991, 0.0011578685, 0.0028700316, -0.006390831, -3.761519e-05, 0.008148598, -0.008594844, -0.0032554928, 0.023757387, 0.004265199, -0.0021891752, -0.009133516, 0.0076680826, 0.0072146454, -0.013836846, 0.0074097016, 0.004119025, -0.0001789946, -0.0037048566, -0.004994813, -0.002755111, 0.0096726, 0.005925477, 0.0013712348, -0.006437224, -0.01613069, 0.0070293997, 0.0033999016, 7.817077e-05, -0.0038728807, -0.0064734234, 0.006887726, -0.006210811, 0.000538582, -0.0050113765, -0.0073163887, -0.008655408, -0.001565167]\n"
     ]
    }
   ],
   "source": [
    "print(len(test_embed))\n",
    "print(test_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que cette variable test contient l'embedding de notre simple phrase; il s'agit d'un vecteur de chiffres qui repr√©sente la version 'num√©rique' de notre phrase.  \n",
    "\n",
    "Ce vecteur est de `dimension 3584` (colonnes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Utilisateurs\\Nelly\\Travail_Perso\\python-envs\\genai-test2\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "# cr√©ation de l'embedding pour tous nos documents, on le stocke ici en m√©moire plut√¥t\n",
    "# qu'une vrai base de donn√©es de vecteurs pour les besoins de l'exemple\n",
    "\n",
    "db = DocArrayInMemorySearch.from_documents(documents=docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Utiliser la base de donn√©es pour effectuer une recherche\n",
    "\n",
    "query = 'Please suggest a shirt with sunblocking'\n",
    "srch = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(srch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=': 709\n",
      "name: Sunrise Tee\n",
      "description: Stay cool, comfortable and dry on the hottest days in our women's UV-protective button down shirt. The lightweight, high-performance fabric wicks away moisture and dries quickly.\n",
      "\n",
      "Size & Fit\n",
      "Slightly Fitted: Softly shapes the body. Falls at hip.\n",
      "\n",
      "Why We Love It\n",
      "Our lightest hot-weather shirt lets you beat the heat. Originally designed for fishing, it's also a great choice for travel thanks to its wrinkle-free fabric and built-in sun protection with a rating of UPF 50+.\n",
      "\n",
      "Fabric & Care\n",
      "Lightweight performance synthetic wicks moisture, resists wrinkles and dries fast. Shell: 71% nylon, 29% polyester. Cape lining: 100% polyester. Machine wash and dry.\n",
      "\n",
      "Additional Features\n",
      "Built-in SunSmart‚Ñ¢ UPF 50+ rated ‚Äì the highest rated sun protection possible. The high-performance fabric keeps you cool and comfortable by wicking perspiration away. Smoother buttons, low-profile pockets and side shaping for a flattering fit. Front and back cape venting. Two front pockets, tool tabs and eyewear loop. Wrinkle free. Imported.\n",
      "\n",
      "Sun Protection That Won't Wear Off\n",
      "Our' metadata={'source': '..\\\\data\\\\csv\\\\retrieval-outdoor-clothing.csv', 'row': 709}\n"
     ]
    }
   ],
   "source": [
    "print(srch[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir utiliser cela afin de faire un question-answer avec nos documents, nous devons cr√©er un retriever.  \n",
    "\n",
    "Il s'agit d'une interface qui peut utiliser n'importe quelle m√©thode sous-jacente et capable de r√©pondre √† une question donn√©e en renvoyant des documents.  \n",
    "La recherche de vecteurs et les embeddings correspondent √† l'une des m√©thodes possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cr√©er un retriever\n",
    "retriever = db.as_retriever()\n",
    "llm = OllamaLLM(model=model_name, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever manuel : on joint tous les documents '√† la main'\n",
    "q_srch = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
    "res = llm.invoke(f\"{q_srch} Question: Please list all your \\\n",
    "shirts with sun protection in a markdown table and summarize each one. Do not mix up \\\n",
    "shirt with other product types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Shirts with Sun Protection\n",
       "\n",
       "| Name | Description | Summary |\n",
       "|---|---|---|\n",
       "| Women's Stop Flies Hoodie |  Uses No Fly Zone Technology to repel ticks, mosquitoes, flies and other biting insects, while also providing UPF 40+ sun protection. Slightly fitted for a soft shape, falls at the hip for added coverage. Breathable and wicks away moisture. | Offers bug protection and sun protection in one garment with a comfortable fit.  | \n",
       "\n",
       "\n",
       "Let me know if you'd like to explore other product categories or have any more questions! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever utilisant une cha√Æne\n",
    "\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Please list all your shirts with sun protection in a markdown table and summarize each one. Do not mix up shirt with other product types.',\n",
       " 'result': \"| Name | Description | Summary |\\n|---|---|---|\\n| Ultimate Two-Layer Polo | This polo shirt is crafted with premium long-staple 100% cotton, honeycomb-knit and garment-washed for softness. It's comfortable enough for the tennis court yet tailored for everyday wear.  | Offers comfort and durability for a variety of settings, ideal for businesses looking to add logos to clothing |\\n| Men's Plaid Tropic Shirt, Short-Sleeve | This shirt is designed for extended travel and offers UPF 50+ protection against harmful UV rays, keeping you cool and dry in hot weather. It features wrinkle-free fabric and ventilation for a comfortable experience.  | Designed specifically for sun protection and warmth in warmer climates.| \\n\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Please list all your \\\n",
    "shirts with sun protection in a markdown table and summarize each one. Do not mix up \\\n",
    "shirt with other product types.'\n",
    "\n",
    "qa_stuff.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Shirts with Sun Protection\n",
       "\n",
       "| Name | Description | Summary |\n",
       "|---|---|---|\n",
       "| Women's Stop Flies Hoodie |  Uses No Fly Zone Technology to repel ticks, mosquitoes, flies and other biting insects, while also providing UPF 40+ sun protection. Slightly fitted for a soft shape, falls at the hip for added coverage. Breathable fabric with wrist and cuff that won't ride up and thumbhole for added coverage. |  Offers bug protection and sun protection in one garment. Lightweight, comfortable, and stylish. | \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='evaluation'>5. Evaluation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mod√®les de langages peuvent parfois proposer des r√©ponses pas tout √† fait exactes lorsqu'ils sont interrog√©s.  \n",
    "Pour construire des applicatiosns robustes, il nous faut trouver un moyen d'√©valuer les r√©ponses : cela peut se faire manuellement (√† l'oeil), mais √©galement en utilisant un LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:2b-instruct-q4_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e un loader LangChain √† partir du fichier de donn√©es\n",
    "fichier = Path('../data/csv-txt/langchain/retrieval-outdoor-clothing.csv')\n",
    "loader = CSVLoader(file_path=fichier, encoding='utf8')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Utilisateurs\\Nelly\\Travail_Perso\\python-envs\\genai-test2\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(model=model_name)\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings\n",
    "    ).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=model_name, temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={'document_separator':'\"<<<<>>>>>\"'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant utiliser des exemples de donn√©es sur lesquels on souhaite effectuer une √©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=': 10\n",
      "name: Cozy Comfort Pullover Set, Stripe\n",
      "description: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\n",
      "\n",
      "Size & Fit\n",
      "- Pants are Favorite Fit: Sits lower on the waist.\n",
      "- Relaxed Fit: Our most generous fit sits farthest from the body.\n",
      "\n",
      "Fabric & Care\n",
      "- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\n",
      "\n",
      "Additional Features\n",
      "- Relaxed fit top with raglan sleeves and rounded hem.\n",
      "- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\n",
      "\n",
      "Imported.' metadata={'source': '..\\\\data\\\\csv-txt\\\\langchain\\\\retrieval-outdoor-clothing.csv', 'row': 10}\n",
      "-----\n",
      "page_content=': 11\n",
      "name: Ultra-Lofty 850 Stretch Down Hooded Jacket\n",
      "description: This technical stretch down jacket from our DownTek collection is sure to keep you warm and comfortable with its full-stretch construction providing exceptional range of motion. With a slightly fitted style that falls at the hip and best with a midweight layer, this jacket is suitable for light activity up to 20¬∞ and moderate activity up to -30¬∞. The soft and durable 100% polyester shell offers complete windproof protection and is insulated with warm, lofty goose down. Other features include welded baffles for a no-stitch construction and excellent stretch, an adjustable hood, an interior media port and mesh stash pocket and a hem drawcord. Machine wash and dry. Imported.' metadata={'source': '..\\\\data\\\\csv-txt\\\\langchain\\\\retrieval-outdoor-clothing.csv', 'row': 11}\n"
     ]
    }
   ],
   "source": [
    "print(data[10])\n",
    "print('-----')\n",
    "print(data[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples de paires question-r√©ponse manuels pour les documents 10 et 11 vus plus haut\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        'query': 'Do the Cozy Comfort Pullover Set have side pockets?',\n",
    "        'answer': 'yes'\n",
    "    },\n",
    "    {\n",
    "        'query': 'What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?',\n",
    "        'answer': 'The DownTek collection'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les exemples cr√©√©s plus haut impliquent de choisir manuellement nos exemples (documents 10 et 11), de cr√©er des questions et de v√©rifier les bonnes r√©ponses √† ces questions. Ensuite, on fournit ces √©l√©ments au mod√®le pour pouvoir l'√©valuer.  \n",
    "\n",
    "Cette technique est relativement s√ªre, mais ne permet pas de passer √† l'√©chelle.  \n",
    "\n",
    "Si l'on veut aller plus loin, une possibilit√© est `d'utiliser une autre cha√Æne avec un LLM en tant qu'√©valuateur`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain, QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAGenerateChain utilise les documents qu'on lui donne en entr√©e pour cr√©er des paires de question-r√©ponse\n",
    "example_generated_chain = QAGenerateChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Utilisateurs\\Nelly\\Travail_Perso\\python-envs\\genai-test2\\Lib\\site-packages\\langchain\\chains\\llm.py:367: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_examples = example_generated_chain.apply_and_parse([{'doc': t} for t in data[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qa_pairs': {'query': \"What is the approximate weight of a pair of Women's Campside Oxfords?\", 'answer': \"The approximate weight of a pair of Women's Campside Oxfords is 1 lb.1 oz. per pair. \"}}\n"
     ]
    }
   ],
   "source": [
    "print(generated_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajoutons ces exemples g√©n√©r√©s √† ceux cr√©√©s manuellement\n",
    "\n",
    "examples += [d['qa_pairs'] for d in generated_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Do the Cozy Comfort Pullover Set have side pockets?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Do the Cozy Comfort Pullover Set have side pockets?\",\n",
      "  \"context\": \"- Stretch-paneled ‚Äúshove it‚Äù pocket provides quick access to trail essentials.\\\"<<<<>>>>>\\\": 69\\nname: Women's Camo Mountain Fleece Vest\\ndescription: Designed for unbeatable warmth and plush comfort, our camouflage heavyweight Sherpa-Fleece Vest looks great from mountain peak to base lodge. Why We Love It: We turned to our archives for inspiration when designing this extraplush sherpa fleece. It's the perfect layer for cold days on the mountain, gathering around the campfire, or running into town. \\r\\n\\r\\nFabric & Care: Thick and cozy 300-weight sherpa fleece provides serious warmth during outdoor excursions. 100% polyester. Machine wash and dry. \\r\\n\\r\\nAdditional Features: Elastic at arm holes and hem for flexible comfort and to block out wind. Features our classic Mount Katahdin logo. Zippered hand pockets.\\\"<<<<>>>>>\\\"This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant, providing SPF 50+ sun protection and blocking 98% of the sun's harmful rays.\\\"<<<<>>>>>\\\"If you're looking for the best kayak for you, watch the video for the Kayak Buying Guide. This guide will help you find the perfect kayak for short paddles to multiday adventures.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n- Stretch-paneled ‚Äúshove it‚Äù pocket provides quick access to trail essentials.\\\"<<<<>>>>>\\\": 69\\nname: Women's Camo Mountain Fleece Vest\\ndescription: Designed for unbeatable warmth and plush comfort, our camouflage heavyweight Sherpa-Fleece Vest looks great from mountain peak to base lodge. Why We Love It: We turned to our archives for inspiration when designing this extraplush sherpa fleece. It's the perfect layer for cold days on the mountain, gathering around the campfire, or running into town. \\r\\n\\r\\nFabric & Care: Thick and cozy 300-weight sherpa fleece provides serious warmth during outdoor excursions. 100% polyester. Machine wash and dry. \\r\\n\\r\\nAdditional Features: Elastic at arm holes and hem for flexible comfort and to block out wind. Features our classic Mount Katahdin logo. Zippered hand pockets.\\\"<<<<>>>>>\\\"This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant, providing SPF 50+ sun protection and blocking 98% of the sun's harmful rays.\\\"<<<<>>>>>\\\"If you're looking for the best kayak for you, watch the video for the Kayak Buying Guide. This guide will help you find the perfect kayak for short paddles to multiday adventures.\\nHuman: Do the Cozy Comfort Pullover Set have side pockets?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOllama] [1.35s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The provided text does not mention a \\\"Cozy Comfort Pullover Set\\\". \\n\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"gemma2:2b-instruct-q4_1\",\n",
      "          \"created_at\": \"2024-10-16T09:49:13.23805Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1342714300,\n",
      "          \"load_duration\": 36968800,\n",
      "          \"prompt_eval_count\": 334,\n",
      "          \"prompt_eval_duration\": 708346000,\n",
      "          \"eval_count\": 17,\n",
      "          \"eval_duration\": 593947000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The provided text does not mention a \\\"Cozy Comfort Pullover Set\\\". \\n\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"gemma2:2b-instruct-q4_1\",\n",
      "              \"created_at\": \"2024-10-16T09:49:13.23805Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1342714300,\n",
      "              \"load_duration\": 36968800,\n",
      "              \"prompt_eval_count\": 334,\n",
      "              \"prompt_eval_duration\": 708346000,\n",
      "              \"eval_count\": 17,\n",
      "              \"eval_duration\": 593947000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-513a274e-b47a-462a-9153-04ab3151b304-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 334,\n",
      "              \"output_tokens\": 17,\n",
      "              \"total_tokens\": 351\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] [1.35s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The provided text does not mention a \\\"Cozy Comfort Pullover Set\\\". \\n\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] [1.35s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"The provided text does not mention a \\\"Cozy Comfort Pullover Set\\\". \\n\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA] [1.87s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"The provided text does not mention a \\\"Cozy Comfort Pullover Set\\\". \\n\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The provided text does not mention a \"Cozy Comfort Pullover Set\". \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(examples[0]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions = qa.apply(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': 'GRADE: INCORRECT \\n'},\n",
       " {'results': 'GRADE: CORRECT \\n'},\n",
       " {'results': 'GRADE: INCORRECT \\n'},\n",
       " {'results': 'GRADE: INCORRECT \\n'},\n",
       " {'results': 'GRADE: INCORRECT \\n'},\n",
       " {'results': 'GRADE: INCORRECT \\n'},\n",
       " {'results': 'GRADE: INCORRECT \\n'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple 0 -------> : \n",
      "Question:  Do the Cozy Comfort Pullover Set have side pockets?\n",
      "R√©ponse attendue:  yes\n",
      "R√©ponse pr√©dite:  The provided text does not mention a \"Cozy Comfort Pullover Set\". \n",
      "\n",
      "Note pr√©dite:  GRADE: INCORRECT \n",
      "\n",
      "Exemple 1 -------> : \n",
      "Question:  What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\n",
      "R√©ponse attendue:  The DownTek collection\n",
      "R√©ponse pr√©dite:  This text does not contain information about the Ultra-Lofty 850 Stretch Down Hooded Jacket or any collections it might be part of. \n",
      "\n",
      "Note pr√©dite:  GRADE: CORRECT \n",
      "\n",
      "Exemple 2 -------> : \n",
      "Question:  What is the approximate weight of a pair of Women's Campside Oxfords?\n",
      "R√©ponse attendue:  The approximate weight of a pair of Women's Campside Oxfords is 1 lb.1 oz. per pair. \n",
      "R√©ponse pr√©dite:  This document does not contain information about Women's Campside Oxfords. \n",
      "\n",
      "Note pr√©dite:  GRADE: INCORRECT \n",
      "\n",
      "Exemple 3 -------> : \n",
      "Question:  What is the construction of the Recycled Waterhog dog mat? \n",
      "R√©ponse attendue:  The Recycled Waterhog dog mat features a 24 oz. polyester fabric made from 94% recycled materials and has a rubber backing.\n",
      "R√©ponse pr√©dite:  This document does not contain information about a \"Recycled Waterhog dog mat\". \n",
      "\n",
      "Note pr√©dite:  GRADE: INCORRECT \n",
      "\n",
      "Exemple 4 -------> : \n",
      "Question:  What is the UPF rating of the fabric in this swimsuit? \n",
      "R√©ponse attendue:  The fabric has a UPF 50+ rating, which provides the highest rated sun protection possible. \n",
      "R√©ponse pr√©dite:  The provided text states that the fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.  It does not provide a UPF rating. \n",
      "\n",
      "Note pr√©dite:  GRADE: INCORRECT \n",
      "\n",
      "Exemple 5 -------> : \n",
      "Question:  What is the UPF rating of this swim top?\n",
      "R√©ponse attendue:  The UPF rating of this swim top is UPF 50+. \n",
      "R√©ponse pr√©dite:  The provided text states that the swim trunks have a UPF 50+ rating. \n",
      "\n",
      "Note pr√©dite:  GRADE: INCORRECT \n",
      "\n",
      "Exemple 6 -------> : \n",
      "Question:  What is the material of the EcoFlex 3L Storm Pants?\n",
      "R√©ponse attendue:  The EcoFlex 3L Storm Pants are made from 100% nylon, exclusive of trim. \n",
      "R√©ponse pr√©dite:  I don't know. The provided text does not mention the material of the EcoFlex 3L Storm Pants. \n",
      "\n",
      "Note pr√©dite:  GRADE: INCORRECT \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check des r√©sultats\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    print(f'Exemple {i} -------> : ')\n",
    "    print('Question: ', predictions[i]['query'])\n",
    "    print('R√©ponse attendue: ', predictions[i]['answer'])\n",
    "    print('R√©ponse pr√©dite: ', predictions[i]['result'])\n",
    "    print('Note pr√©dite: ', graded_outputs[i]['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='agents'>6. Agents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les agents sont des outils bas√©s sur les LLMs, mais que l'on √©quipe afin qu'ils puissent interagir de fa√ßon autonome avec d'autres outils : on parle alors de 'moteur de raisonnement', et il s'agit de la meilleure mani√®re d'utiliser les LLMs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:2b-instruct-q4_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain_experimental.agents.agent_toolkits.python.base import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_experimental.utilities import PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model = model_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on charge les outils 'llm-math' (une cha√Æne qui combine llm + calculatrice), et 'wikipedia' pour requ√™ter la base Wikip√©dia\n",
    "tools = load_tools(['llm-math', 'wikipedia'], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nelly\\AppData\\Local\\Temp\\ipykernel_15364\\235751029.py:3: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# Initialiser un agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"What is the 25% of 300?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "agent(question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un agent avec Python\n",
    "\n",
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nelly\\AppData\\Local\\Temp\\ipykernel_15364\\340590393.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  agent.run(f'''Sort these customers by last name and then first name and print the output: {customer_list}''')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I need to sort the list of customers by last name and then first name. \n",
      "Action: Python_REPL\n",
      "Action Input: \n",
      "```python\n",
      "customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
      "\n",
      "customers.sort(key=lambda x: (x[1], x[0]))\n",
      "print(customers) \n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']] \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(f'''Sort these customers by last name and then first name and print the output: {customer_list}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOllama] [7.37s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thought: I need to sort the list of customers by last name and then first name. \\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\n\\ncustomers.sort(key=lambda x: (x[1], x[0]))\\nprint(customers) \\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"gemma2:2b-instruct-q4_1\",\n",
      "          \"created_at\": \"2024-10-16T11:06:03.5885264Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 7361936700,\n",
      "          \"load_duration\": 2566698200,\n",
      "          \"prompt_eval_count\": 336,\n",
      "          \"prompt_eval_duration\": 636037000,\n",
      "          \"eval_count\": 111,\n",
      "          \"eval_duration\": 4157129000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Thought: I need to sort the list of customers by last name and then first name. \\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\n\\ncustomers.sort(key=lambda x: (x[1], x[0]))\\nprint(customers) \\n```\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"gemma2:2b-instruct-q4_1\",\n",
      "              \"created_at\": \"2024-10-16T11:06:03.5885264Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 7361936700,\n",
      "              \"load_duration\": 2566698200,\n",
      "              \"prompt_eval_count\": 336,\n",
      "              \"prompt_eval_duration\": 636037000,\n",
      "              \"eval_count\": 111,\n",
      "              \"eval_duration\": 4157129000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8ac93b3d-ecc3-49c5-864a-cd8e57905c58-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 336,\n",
      "              \"output_tokens\": 111,\n",
      "              \"total_tokens\": 447\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [7.37s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Thought: I need to sort the list of customers by last name and then first name. \\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\n\\ncustomers.sort(key=lambda x: (x[1], x[0]))\\nprint(customers) \\n```\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] Entering Tool run with input:\n",
      "\u001b[0m\"```python\n",
      "customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
      "\n",
      "customers.sort(key=lambda x: (x[1], x[0]))\n",
      "print(customers) \n",
      "```\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] [2ms] Exiting Tool run with output:\n",
      "\u001b[0m\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"Thought: I need to sort the list of customers by last name and then first name. \\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\n\\ncustomers.sort(key=lambda x: (x[1], x[0]))\\nprint(customers) \\n```\\nObservation: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:Thought: I need to sort the list of customers by last name and then first name. \\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\n\\ncustomers.sort(key=lambda x: (x[1], x[0]))\\nprint(customers) \\n```\\nObservation: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOllama] [2.65s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thought: I now know the final answer.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']] \\n\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"gemma2:2b-instruct-q4_1\",\n",
      "          \"created_at\": \"2024-10-16T11:06:06.244323Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2647384300,\n",
      "          \"load_duration\": 37737300,\n",
      "          \"prompt_eval_count\": 494,\n",
      "          \"prompt_eval_duration\": 325965000,\n",
      "          \"eval_count\": 60,\n",
      "          \"eval_duration\": 2281585000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Thought: I now know the final answer.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']] \\n\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"gemma2:2b-instruct-q4_1\",\n",
      "              \"created_at\": \"2024-10-16T11:06:06.244323Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2647384300,\n",
      "              \"load_duration\": 37737300,\n",
      "              \"prompt_eval_count\": 494,\n",
      "              \"prompt_eval_duration\": 325965000,\n",
      "              \"eval_count\": 60,\n",
      "              \"eval_duration\": 2281585000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a6537a05-f7f7-4413-ab0b-47662f8fc712-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 494,\n",
      "              \"output_tokens\": 60,\n",
      "              \"total_tokens\": 554\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Thought: I now know the final answer.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']] \\n\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [10.03s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n",
    "agent.run(f'''Sort these customers by last name and then first name and print the output: {customer_list}''')\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des outils personnalis√©s pour interagir avec les agents\n",
    "# Exemple : cr√©ation d'un outil qui nous indique la date\n",
    "\n",
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool\n",
    "def current_date(text: str) -> str:\n",
    "    '''\n",
    "    Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\n",
    "    '''\n",
    "    return str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools + [current_date],\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run('What is the current date?')"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Nelly Traore"
   }
  ],
  "kernelspec": {
   "display_name": "genai-test2",
   "language": "python",
   "name": "genai-test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
